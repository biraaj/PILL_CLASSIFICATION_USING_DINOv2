{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6m-iS9292PO",
        "outputId": "e3dda46a-56a5-4fac-eb17-c1746c0709e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformers is already installed.\n",
            "timm is already installed.\n",
            "torch is already installed.\n",
            "torchvision is already installed.\n",
            "pandas is already installed.\n",
            "numpy is already installed.\n",
            "Installing scikit-learn...\n",
            "Installing Pillow...\n",
            "All required packages are checked and installed if needed.\n"
          ]
        }
      ],
      "source": [
        "import importlib.util\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# List of packages to check and install\n",
        "packages = ['transformers', 'timm', 'torch', 'torchvision', 'pandas', 'numpy', 'scikit-learn', 'Pillow']\n",
        "\n",
        "def is_package_installed(package):\n",
        "    \"\"\"Check if a package is installed.\"\"\"\n",
        "    return importlib.util.find_spec(package) is not None\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package using pip.\"\"\"\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "\n",
        "# Check and install each package\n",
        "for package in packages:\n",
        "    if not is_package_installed(package):\n",
        "        print(f\"Installing {package}...\")\n",
        "        install_package(package)\n",
        "    else:\n",
        "        print(f\"{package} is already installed.\")\n",
        "\n",
        "print(\"All required packages are checked and installed if needed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9s2z-E292PO",
        "outputId": "2e8639d3-a79d-4cea-9b92-7af2c2d6f1c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Torchvision version: 0.21.0+cu124\n",
            "Transformers version: 4.51.3\n",
            "TIMM version: 1.0.15\n",
            "Pandas version: 2.2.2\n",
            "NumPy version: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import timm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from PIL import Image\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"TIMM version:\", timm.__version__)\n",
        "print(\"Pandas version:\", pd.__version__)\n",
        "print(\"NumPy version:\", np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17tB-n_H92PP",
        "outputId": "d4b3d025-4cb4-447b-ba51-395240a8c01a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists at ePillID_data.zip. Skipping download.\n",
            "Data directory ePillID_data already has files. Skipping extraction.\n",
            "Files in data directory: ['ePillID_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "data_dir = Path('./ePillID_data')\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Download the file with the correct direct link\n",
        "zip_path = Path('./ePillID_data.zip')\n",
        "url = 'https://github.com/usuyama/ePillID-benchmark/releases/download/ePillID_data_v1.0/ePillID_data.zip'\n",
        "\n",
        "# Only download if the file doesn't exist\n",
        "if not zip_path.exists():\n",
        "    print(\"Downloading dataset...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Download complete. File size: {zip_path.stat().st_size / (1024*1024):.2f} MB\")\n",
        "    else:\n",
        "        print(f\"ERROR: Failed to download file. Status code: {response.status_code}\")\n",
        "else:\n",
        "    print(f\"File already exists at {zip_path}. Skipping download.\")\n",
        "\n",
        "# Check if file exists and has reasonable size before extracting\n",
        "if zip_path.exists():\n",
        "    if zip_path.stat().st_size > 1000000:  # More than 1MB\n",
        "        # Only extract if the data directory is empty\n",
        "        if not os.listdir(data_dir) if data_dir.exists() else True:\n",
        "            print(\"Extracting ZIP file...\")\n",
        "            try:\n",
        "                with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(data_dir)\n",
        "                print(\"Extraction complete!\")\n",
        "            except zipfile.BadZipFile:\n",
        "                print(\"ERROR: The file is not a valid ZIP file.\")\n",
        "        else:\n",
        "            print(f\"Data directory {data_dir} already has files. Skipping extraction.\")\n",
        "    else:\n",
        "        print(\"ERROR: The file seems too small to be valid.\")\n",
        "else:\n",
        "    print(\"ZIP file not found. Cannot extract.\")\n",
        "\n",
        "# Check the contents\n",
        "if data_dir.exists() and os.listdir(data_dir):\n",
        "    print(f\"Files in data directory: {os.listdir(data_dir)}\")\n",
        "else:\n",
        "    print(\"Data directory is empty or not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8ydOV7092PP",
        "outputId": "00e0952c-936a-4f8d-adb4-71e52e9de100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     images             pilltype_id  label_code_id  prod_code_id  is_ref  \\\n",
            "0     0.jpg  51285-0092-87_BE305F72          51285            92   False   \n",
            "1    10.jpg  00093-0148-01_4629A34D             93           148   False   \n",
            "2   100.jpg  00093-7248-06_7829BC3D             93          7248   False   \n",
            "3  1003.jpg  00093-0928-06_6926B4E5             93           928   False   \n",
            "4  1004.jpg  50111-0459-01_1C300E70          50111           459   False   \n",
            "\n",
            "   is_front  is_new                      image_path                   label  \n",
            "0     False   False     fcn_mix_weight/dc_224/0.jpg  51285-0092-87_BE305F72  \n",
            "1     False   False    fcn_mix_weight/dc_224/10.jpg  00093-0148-01_4629A34D  \n",
            "2      True   False   fcn_mix_weight/dc_224/100.jpg  00093-7248-06_7829BC3D  \n",
            "3     False   False  fcn_mix_weight/dc_224/1003.jpg  00093-0928-06_6926B4E5  \n",
            "4      True   False  fcn_mix_weight/dc_224/1004.jpg  50111-0459-01_1C300E70  \n",
            "Index(['images', 'pilltype_id', 'label_code_id', 'prod_code_id', 'is_ref',\n",
            "       'is_front', 'is_new', 'image_path', 'label'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Update data_path to the nested directory\n",
        "data_path = \"./ePillID_data/ePillID_data\"\n",
        "metadata_file = os.path.join(data_path, \"all_labels.csv\")\n",
        "metadata = pd.read_csv(metadata_file)\n",
        "print(metadata.head())\n",
        "print(metadata.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj2Z0vrN92PQ",
        "outputId": "3b8b84a7-45ab-4b3e-de2f-f825912f4e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference images: 9804\n",
            "Consumer images: 3728\n",
            "\n",
            "Reference images by directory:\n",
            "- segmented_nih_pills_224 = 7804 images\n",
            "- fcn_mix_weight/dr_224 = 2000 images\n",
            "\n",
            "Consumer images by directory:\n",
            "- fcn_mix_weight/dc_224 = 3728 images\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Define transformations for ViT\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Custom Dataset\n",
        "class EPillIDDataset(Dataset):\n",
        "    def __init__(self, metadata, transform=None):\n",
        "        self.metadata = metadata\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.metadata.iloc[idx][\"image_path\"]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.metadata.iloc[idx][\"pilltype_id\"]\n",
        "        return image, label\n",
        "\n",
        "# Split reference and consumer datasets\n",
        "reference_metadata = metadata[metadata[\"is_ref\"] == True]\n",
        "consumer_metadata = metadata[metadata[\"is_ref\"] == False]\n",
        "reference_dataset = EPillIDDataset(reference_metadata, transform=transform)\n",
        "consumer_dataset = EPillIDDataset(consumer_metadata, transform=transform)\n",
        "print(f\"Reference images: {len(reference_dataset)}\")\n",
        "print(f\"Consumer images: {len(consumer_dataset)}\")\n",
        "\n",
        "# Add directory analysis to show distribution of images\n",
        "print(\"\\nReference images by directory:\")\n",
        "ref_dir_counts = defaultdict(int)\n",
        "for img_path in reference_metadata[\"image_path\"]:\n",
        "    # Extract just the directory part (not full path to each file)\n",
        "    directory = os.path.dirname(img_path)\n",
        "    ref_dir_counts[directory] += 1\n",
        "\n",
        "# Print summary of reference image directories\n",
        "for dir_path, count in sorted(ref_dir_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"- {dir_path} = {count} images\")\n",
        "\n",
        "print(\"\\nConsumer images by directory:\")\n",
        "consumer_dir_counts = defaultdict(int)\n",
        "for img_path in consumer_metadata[\"image_path\"]:\n",
        "    # Extract just the directory part (not full path to each file)\n",
        "    directory = os.path.dirname(img_path)\n",
        "    consumer_dir_counts[directory] += 1\n",
        "\n",
        "# Print summary of consumer image directories\n",
        "for dir_path, count in sorted(consumer_dir_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"- {dir_path} = {count} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMFHApE592PQ",
        "outputId": "834b4b86-dbbd-49d9-f40e-978d35c4676e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total valid images: 13532\n",
            "Valid reference images: 9804\n",
            "Valid consumer images: 3728\n"
          ]
        }
      ],
      "source": [
        "# Cell 6 - update image paths in meta data to point to correct directories\n",
        "# Update image paths in metadata\n",
        "def update_image_paths(metadata):\n",
        "    data_path = \"./ePillID_data/ePillID_data\"\n",
        "\n",
        "    # Define directories\n",
        "    dr_224_dir = os.path.join(data_path, \"classification_data\", \"fcn_mix_weight\", \"dr_224\")\n",
        "    segmented_dir = os.path.join(data_path, \"classification_data\", \"segmented_nih_pills_224\")\n",
        "    dc_224_dir = os.path.join(data_path, \"classification_data\", \"fcn_mix_weight\", \"dc_224\")\n",
        "\n",
        "    # Update paths\n",
        "    updated_metadata = metadata.copy()\n",
        "\n",
        "    # For each row in the dataframe\n",
        "    for idx, row in updated_metadata.iterrows():\n",
        "        img_name = row['images']\n",
        "\n",
        "        if row['is_ref']:\n",
        "            # Try both reference directories\n",
        "            path1 = os.path.join(dr_224_dir, img_name)\n",
        "            path2 = os.path.join(segmented_dir, img_name)\n",
        "\n",
        "            if os.path.exists(path1):\n",
        "                updated_metadata.at[idx, 'image_path'] = path1\n",
        "            elif os.path.exists(path2):\n",
        "                updated_metadata.at[idx, 'image_path'] = path2\n",
        "            else:\n",
        "                updated_metadata.at[idx, 'image_path'] = None\n",
        "        else:\n",
        "            # Consumer images\n",
        "            path = os.path.join(dc_224_dir, img_name)\n",
        "            if os.path.exists(path):\n",
        "                updated_metadata.at[idx, 'image_path'] = path\n",
        "            else:\n",
        "                updated_metadata.at[idx, 'image_path'] = None\n",
        "\n",
        "    # Filter out images with no valid path\n",
        "    valid_metadata = updated_metadata[updated_metadata['image_path'].notna()].copy()\n",
        "\n",
        "    print(f\"Total valid images: {len(valid_metadata)}\")\n",
        "    print(f\"Valid reference images: {sum(valid_metadata['is_ref'] == True)}\")\n",
        "    print(f\"Valid consumer images: {sum(valid_metadata['is_ref'] == False)}\")\n",
        "\n",
        "    return valid_metadata\n",
        "\n",
        "# Update metadata with correct paths\n",
        "fixed_metadata = update_image_paths(metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35eepaoj92PQ",
        "outputId": "39fec5ff-ce38-4b95-c935-aa172cfdfdc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique pill types: 4902\n",
            "Valid reference images: 9804\n",
            "Valid consumer images: 3728\n",
            "\n",
            "Label distribution:\n",
            "Unique labels in reference set: 4902\n",
            "Unique labels in consumer set: 960\n",
            "Labels in both sets: 960\n",
            "\n",
            "Sample reference pill:\n",
            "           pilltype_id  label_encoded\n",
            "00002-3228-30_391E1C80              0\n",
            "\n",
            "Sample consumer pill:\n",
            "           pilltype_id  label_encoded\n",
            "51285-0092-87_BE305F72           2089\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Label encoding with the combined dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "fixed_metadata['label_encoded'] = label_encoder.fit_transform(fixed_metadata['pilltype_id'])\n",
        "num_classes = len(fixed_metadata['pilltype_id'].unique())\n",
        "print(f\"Number of unique pill types: {num_classes}\")\n",
        "\n",
        "# Split into reference and consumer datasets again\n",
        "reference_metadata = fixed_metadata[fixed_metadata[\"is_ref\"] == True]\n",
        "consumer_metadata = fixed_metadata[fixed_metadata[\"is_ref\"] == False]\n",
        "print(f\"Valid reference images: {len(reference_metadata)}\")\n",
        "print(f\"Valid consumer images: {len(consumer_metadata)}\")\n",
        "\n",
        "# Print statistics about label distribution\n",
        "print(\"\\nLabel distribution:\")\n",
        "ref_labels = set(reference_metadata['label_encoded'])\n",
        "con_labels = set(consumer_metadata['label_encoded'])\n",
        "common_labels = ref_labels.intersection(con_labels)\n",
        "print(f\"Unique labels in reference set: {len(ref_labels)}\")\n",
        "print(f\"Unique labels in consumer set: {len(con_labels)}\")\n",
        "print(f\"Labels in both sets: {len(common_labels)}\")\n",
        "\n",
        "# Print information about a few examples from each set\n",
        "print(\"\\nSample reference pill:\")\n",
        "print(reference_metadata[['pilltype_id', 'label_encoded']].head(1).to_string(index=False))\n",
        "print(\"\\nSample consumer pill:\")\n",
        "print(consumer_metadata[['pilltype_id', 'label_encoded']].head(1).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcRKAfkb92PR",
        "outputId": "d68aba69-8561-4207-c81c-348d34eb8b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution of samples per class:\n",
            "- Minimum samples per class: 1\n",
            "- Maximum samples per class: 5\n",
            "- Average samples per class: 3.88\n",
            "- Classes with only 1 sample: 54\n",
            "- Total unique classes: 960\n",
            "\n",
            "Training samples (consumer): 2982\n",
            "Validation samples (consumer): 746\n",
            "Reference samples (gallery): 9804\n",
            "\n",
            "Unique classes in training: 941\n",
            "Unique classes in validation: 541\n",
            "Unique classes in reference: 4902\n",
            "\n",
            "Classes in both training and validation: 522\n",
            "Classes in both training and reference: 941\n",
            "Classes in both validation and reference: 541\n",
            "\n",
            "Classes that appear only in consumer images: 0\n",
            "Classes that appear only in reference images: 3942\n",
            "\n",
            "Classes with both reference and consumer images: 960\n",
            "\n",
            "Warning: 19 classes appear in validation but not training\n",
            "- Of these, 19 have reference images\n",
            "- Total samples for validation-only classes: 24\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Data split for training and validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# For ePillID benchmark, we should:\n",
        "# 1. Use consumer images for training/validation\n",
        "# 2. Use reference images as the \"gallery\" for retrieval evaluation\n",
        "\n",
        "# First, analyze the class distribution to understand our dataset\n",
        "class_counts = consumer_metadata['label_encoded'].value_counts()\n",
        "print(f\"Distribution of samples per class:\")\n",
        "print(f\"- Minimum samples per class: {class_counts.min()}\")\n",
        "print(f\"- Maximum samples per class: {class_counts.max()}\")\n",
        "print(f\"- Average samples per class: {class_counts.mean():.2f}\")\n",
        "print(f\"- Classes with only 1 sample: {sum(class_counts == 1)}\")\n",
        "print(f\"- Total unique classes: {len(class_counts)}\")\n",
        "\n",
        "# Given the high number of classes and many with single samples,\n",
        "# we'll use a simple random split rather than stratification\n",
        "train_data, val_data = train_test_split(\n",
        "    consumer_metadata,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        "    # No stratify parameter - using random split instead\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining samples (consumer): {len(train_data)}\")\n",
        "print(f\"Validation samples (consumer): {len(val_data)}\")\n",
        "print(f\"Reference samples (gallery): {len(reference_metadata)}\")\n",
        "\n",
        "# Check how many classes are represented in each split\n",
        "train_classes = set(train_data['label_encoded'])\n",
        "val_classes = set(val_data['label_encoded'])\n",
        "ref_classes = set(reference_metadata['label_encoded'])\n",
        "\n",
        "print(f\"\\nUnique classes in training: {len(train_classes)}\")\n",
        "print(f\"Unique classes in validation: {len(val_classes)}\")\n",
        "print(f\"Unique classes in reference: {len(ref_classes)}\")\n",
        "\n",
        "# Check overlap\n",
        "train_val_overlap = train_classes.intersection(val_classes)\n",
        "train_ref_overlap = train_classes.intersection(ref_classes)\n",
        "val_ref_overlap = val_classes.intersection(ref_classes)\n",
        "\n",
        "print(f\"\\nClasses in both training and validation: {len(train_val_overlap)}\")\n",
        "print(f\"Classes in both training and reference: {len(train_ref_overlap)}\")\n",
        "print(f\"Classes in both validation and reference: {len(val_ref_overlap)}\")\n",
        "\n",
        "# Check for classes that appear in consumer but not reference images (or vice versa)\n",
        "consumer_only = (train_classes.union(val_classes)).difference(ref_classes)\n",
        "reference_only = ref_classes.difference(train_classes.union(val_classes))\n",
        "\n",
        "print(f\"\\nClasses that appear only in consumer images: {len(consumer_only)}\")\n",
        "print(f\"Classes that appear only in reference images: {len(reference_only)}\")\n",
        "\n",
        "# For evaluation, we should ensure we only test on classes that have both reference and consumer images\n",
        "print(f\"\\nClasses with both reference and consumer images: {len(train_ref_overlap.union(val_ref_overlap))}\")\n",
        "\n",
        "# Additionally, check how many classes are in validation but not training\n",
        "val_only_classes = val_classes.difference(train_classes)\n",
        "if len(val_only_classes) > 0:\n",
        "    print(f\"\\nWarning: {len(val_only_classes)} classes appear in validation but not training\")\n",
        "\n",
        "    # Check how many of these classes have reference images\n",
        "    val_only_with_ref = val_only_classes.intersection(ref_classes)\n",
        "    print(f\"- Of these, {len(val_only_with_ref)} have reference images\")\n",
        "\n",
        "    # Print sample size for these classes\n",
        "    val_only_samples = val_data[val_data['label_encoded'].isin(val_only_classes)]\n",
        "    print(f\"- Total samples for validation-only classes: {len(val_only_samples)}\")\n",
        "\n",
        "    # Optionally, we could move these to training to avoid \"zero-shot\" evaluation\n",
        "    # But for now, we'll keep them, as this mimics real-world pill identification scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQySMCaO92PR",
        "outputId": "364ff1b4-a58b-45f1-95d5-c436a45c0b21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has 2982 valid images out of 2982 entries\n",
            "Dataset has 746 valid images out of 746 entries\n",
            "Dataset has 9804 valid images out of 9804 entries\n",
            "Training dataset: 2982 images\n",
            "Validation dataset: 746 images\n",
            "Reference dataset: 9804 images\n"
          ]
        }
      ],
      "source": [
        "# Cell 9: Dataset and DataLoader setup with optimized dataset class\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define transformations\n",
        "# For training: add augmentations to improve generalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# For validation and reference: no augmentations, just resizing and normalization\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class OptimizedEPillIDDataset(Dataset):\n",
        "    def __init__(self, metadata, transform=None):\n",
        "        self.metadata = metadata\n",
        "        self.transform = transform\n",
        "        self.valid_indices = []\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Pre-process all valid paths and labels\n",
        "        for idx in range(len(self.metadata)):\n",
        "            img_path = self.metadata.iloc[idx][\"image_path\"]\n",
        "            if os.path.exists(img_path):\n",
        "                self.valid_indices.append(idx)\n",
        "                self.image_paths.append(img_path)\n",
        "                self.labels.append(self.metadata.iloc[idx][\"label_encoded\"])\n",
        "\n",
        "        print(f\"Dataset has {len(self.valid_indices)} valid images out of {len(self.metadata)} entries\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        metadata_idx = self.valid_indices[idx]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, label, metadata_idx\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_path}: {e}\")\n",
        "            placeholder = torch.zeros(3, 224, 224)\n",
        "            return placeholder, label, metadata_idx\n",
        "\n",
        "# Create datasets with the optimized class\n",
        "train_dataset = OptimizedEPillIDDataset(train_data, transform=train_transform)\n",
        "val_dataset = OptimizedEPillIDDataset(val_data, transform=val_transform)\n",
        "reference_dataset = OptimizedEPillIDDataset(reference_metadata, transform=val_transform)\n",
        "\n",
        "print(f\"Training dataset: {len(train_dataset)} images\")\n",
        "print(f\"Validation dataset: {len(val_dataset)} images\")\n",
        "print(f\"Reference dataset: {len(reference_dataset)} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMWHUTBX92PR",
        "outputId": "61358125-1991-4f98-92d0-4b02ac514e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU count: 2 logical processors\n",
            "Available memory: 10.64 GB\n",
            "Training batches: 94 (batch size: 32)\n",
            "Validation batches: 24 (batch size: 32)\n",
            "Reference batches: 307 (batch size: 32)\n",
            "\n",
            "Testing disk read speed (this takes a few seconds)...\n",
            "Disk read speed: 1722.51 MB/s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Timing first batch loading...\n",
            "Time to load first batch: 1.09 seconds\n",
            "Batch shape: torch.Size([32, 3, 224, 224])\n",
            "Labels shape: torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import psutil\n",
        "import time\n",
        "\n",
        "# Print system information for diagnostics\n",
        "print(f\"CPU count: {os.cpu_count()} logical processors\")\n",
        "print(f\"Available memory: {psutil.virtual_memory().available / (1024**3):.2f} GB\")\n",
        "\n",
        "# Configure performance parameters - optimized for Colab\n",
        "batch_size = 32  # Reduced for memory efficiency\n",
        "num_workers = 4  # Reduced to avoid CPU memory overload\n",
        "prefetch_factor = 2  # Reduced to minimize memory usage\n",
        "persistent_workers = True\n",
        "\n",
        "# Create optimized data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True if torch.cuda.is_available() else False,\n",
        "    prefetch_factor=prefetch_factor,\n",
        "    persistent_workers=persistent_workers if num_workers > 0 else False,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True if torch.cuda.is_available() else False,\n",
        "    prefetch_factor=prefetch_factor,\n",
        "    persistent_workers=persistent_workers if num_workers > 0 else False,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "reference_loader = DataLoader(\n",
        "    reference_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True if torch.cuda.is_available() else False,\n",
        "    prefetch_factor=prefetch_factor,\n",
        "    persistent_workers=persistent_workers if num_workers > 0 else False,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# Print information about the data loaders\n",
        "print(f\"Training batches: {len(train_loader)} (batch size: {batch_size})\")\n",
        "print(f\"Validation batches: {len(val_loader)} (batch size: {batch_size})\")\n",
        "print(f\"Reference batches: {len(reference_loader)} (batch size: {batch_size})\")\n",
        "\n",
        "# Check disk I/O performance\n",
        "try:\n",
        "    print(\"\\nTesting disk read speed (this takes a few seconds)...\")\n",
        "    start_time = time.time()\n",
        "    test_size_mb = 100\n",
        "    with open(\"temp_test_file.bin\", \"wb\") as f:\n",
        "        f.write(b'0' * 1024 * 1024 * test_size_mb)\n",
        "\n",
        "    with open(\"temp_test_file.bin\", \"rb\") as f:\n",
        "        start_read = time.time()\n",
        "        f.read()\n",
        "        end_read = time.time()\n",
        "\n",
        "    read_speed = test_size_mb / (end_read - start_read)\n",
        "    print(f\"Disk read speed: {read_speed:.2f} MB/s\")\n",
        "\n",
        "    os.remove(\"temp_test_file.bin\")\n",
        "except Exception as e:\n",
        "    print(f\"Couldn't test disk speed: {e}\")\n",
        "\n",
        "# Check first batch timing\n",
        "print(\"\\nTiming first batch loading...\")\n",
        "start_time = time.time()\n",
        "dataloader_iterator = iter(train_loader)\n",
        "first_batch = next(dataloader_iterator)\n",
        "end_time = time.time()\n",
        "images, labels, indices = first_batch\n",
        "\n",
        "print(f\"Time to load first batch: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Batch shape: {images.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 11: THIS IS TO DEFINE THE MODEL:\n"
      ],
      "metadata": {
        "id": "RzI2shUbArW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define the Vision Transformer model (Swin Transformer)\n",
        "model_name = \"swin_base_patch4_window7_224\"\n",
        "num_classes = len(fixed_metadata['pilltype_id'].unique())  # 4902 pill types\n",
        "\n",
        "# Initialize the model with global pooling\n",
        "model = timm.create_model(\n",
        "    model_name,\n",
        "    pretrained=True,\n",
        "    num_classes=num_classes,\n",
        "    global_pool='avg'  # Add average pooling to flatten features\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "print(f\"Loaded {model_name} with {num_classes} output classes\")\n",
        "\n",
        "# Verify model output shape\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "with torch.no_grad():\n",
        "    output = model(dummy_input)\n",
        "    print(f\"Model output shape: {output.shape}\")  # Should be [1, 4902]\n",
        "\n",
        "# Initialize head weights to reduce initial logit magnitudes\n",
        "with torch.no_grad():\n",
        "    if hasattr(model, 'head') and isinstance(model.head, nn.Linear):\n",
        "        model.head.weight.data.normal_(mean=0.0, std=0.01)\n",
        "        model.head.bias.data.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6Ggh0u8Aq5Q",
        "outputId": "453351b4-b97c-439e-c33a-2bf4f4eeeff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded swin_base_patch4_window7_224 with 4902 output classes\n",
            "Model output shape: torch.Size([1, 4902])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 12: THIS IS TO DEFINE LOSS FUNCTION AND OPTIMIZER"
      ],
      "metadata": {
        "id": "91OEq93XA6Dw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   CrossEntropyLoss is used for classification pre-training.\n",
        "2.   AdamW is suitable for transformer models, with a low learning rate for fine-tuning.\n",
        "3.   The cosine annealing scheduler helps with convergence."
      ],
      "metadata": {
        "id": "45-diS-bBLJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Define scheduler for learning rate decay\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "print(\"Loss function and optimizer initialized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82EZsRTJA9cu",
        "outputId": "89ce9380-6d5d-4b92-d1b6-81fe2add9271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function and optimizer initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 13:\n",
        " THIS IS FOR THE TRAINING LOOP:"
      ],
      "metadata": {
        "id": "k4WtMiP_BaBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   This loop trains for classification, which helps the model learn discriminative features.\n",
        "*   The model is saved when validation accuracy improves.\n",
        "*   Adjust num_epochs based on your computational resources (start with 5–10 epochs).\n",
        "\n"
      ],
      "metadata": {
        "id": "-P1dWPazBdBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.amp import GradScaler, autocast\n",
        "import gc\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, scaler, device, accum_steps=4):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (images, labels, _) in enumerate(tqdm(loader, desc=\"Training\")):\n",
        "        images, labels = images.to(device), labels.to(device, dtype=torch.long)\n",
        "\n",
        "        # Debug shapes\n",
        "        if i == 0:\n",
        "            print(f\"Batch {i+1}: Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
        "\n",
        "        with autocast('cuda'):\n",
        "            outputs = model(images)\n",
        "            if i == 0:\n",
        "                print(f\"Batch {i+1}: Outputs shape: {outputs.shape}\")\n",
        "            if len(outputs.shape) != 2 or outputs.shape[1] != 4902:\n",
        "                raise ValueError(f\"Unexpected output shape: {outputs.shape}, expected [batch_size, 4902]\")\n",
        "            loss = criterion(outputs, labels) / accum_steps\n",
        "\n",
        "        # Check for inf/NaN loss\n",
        "        if not torch.isfinite(loss):\n",
        "            print(f\"Warning: Skipping batch {i+1} due to inf/NaN loss: {loss.item()}\")\n",
        "            optimizer.zero_grad()  # Clear gradients\n",
        "            continue\n",
        "\n",
        "        # Debug loss value\n",
        "        if i == 0:\n",
        "            print(f\"Batch {i+1}: Loss value: {loss.item() * accum_steps}\")\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i + 1) % accum_steps == 0 or (i + 1) == len(loader):\n",
        "            # Unscale gradients\n",
        "            scaler.unscale_(optimizer)\n",
        "\n",
        "            # Gradient clipping with stronger norm\n",
        "            grad_norm = nn_utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "            if i % 10 == 0 or (i + 1) == len(loader):\n",
        "                print(f\"Batch {i+1}: Gradient norm: {grad_norm:.4f}\")\n",
        "\n",
        "            # Check for inf/NaN gradients\n",
        "            has_inf_gradients = False\n",
        "            for param in model.parameters():\n",
        "                if param.grad is not None and not torch.isfinite(param.grad).all():\n",
        "                    has_inf_gradients = True\n",
        "                    break\n",
        "            if has_inf_gradients:\n",
        "                print(f\"Warning: Inf/NaN gradients in batch {i+1}, skipping optimizer step\")\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                continue\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item() * accum_steps\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(loader) if total > 0 else float('inf')\n",
        "    epoch_acc = 100 * correct / total if total > 0 else 0.0\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels, _) in enumerate(tqdm(loader, desc=\"Validation\")):\n",
        "            images, labels = images.to(device), labels.to(device, dtype=torch.long)\n",
        "            with autocast('cuda'):\n",
        "                outputs = model(images)\n",
        "                if len(outputs.shape) != 2 or outputs.shape[1] != 4902:\n",
        "                    raise ValueError(f\"Unexpected output shape: {outputs.shape}, expected [batch_size, 4902]\")\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            if not torch.isfinite(loss):\n",
        "                print(f\"Warning: Skipping validation batch {i+1} due to inf/NaN loss\")\n",
        "                continue\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(loader) if total > 0 else float('inf')\n",
        "    epoch_acc = 100 * correct / total if total > 0 else 0.0\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = GradScaler('cuda')\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_acc = 0.0\n",
        "save_path = \"./best_swin_model.pth\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler, device, accum_steps=4)\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Saved best model with Val Acc: {best_val_acc:.2f}%\")\n",
        "\n",
        "print(\"Training complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jz2iQelBlNZ",
        "outputId": "c09cf0cd-a86d-4a87-c303-a4d3699e3dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/94 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
            "Batch 1: Outputs shape: torch.Size([32, 4902])\n",
            "Batch 1: Loss value: 8.5396728515625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:40<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 94: Gradient norm: 6.8740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 24/24 [00:04<00:00,  5.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 8.2233, Train Acc: 0.17%\n",
            "Val Loss: 7.5748, Val Acc: 0.67%\n",
            "Saved best model with Val Acc: 0.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/94 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
            "Batch 1: Outputs shape: torch.Size([32, 4902])\n",
            "Batch 1: Loss value: 6.8516845703125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:42<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 94: Gradient norm: 8.8354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 24/24 [00:03<00:00,  6.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10\n",
            "Train Loss: 6.3940, Train Acc: 3.55%\n",
            "Val Loss: 6.4321, Val Acc: 3.08%\n",
            "Saved best model with Val Acc: 3.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/94 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
            "Batch 1: Outputs shape: torch.Size([32, 4902])\n",
            "Batch 1: Loss value: 5.01336669921875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|█▎        | 12/94 [00:06<00:35,  2.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Inf/NaN gradients in batch 12, skipping optimizer step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:41<00:00,  2.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 94: Gradient norm: 9.6763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 24/24 [00:03<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10\n",
            "Train Loss: 4.8531, Train Acc: 15.93%\n",
            "Val Loss: 5.5766, Val Acc: 8.71%\n",
            "Saved best model with Val Acc: 8.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/94 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
            "Batch 1: Outputs shape: torch.Size([32, 4902])\n",
            "Batch 1: Loss value: 3.76312255859375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  51%|█████     | 48/94 [00:22<00:20,  2.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Inf/NaN gradients in batch 48, skipping optimizer step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:42<00:00,  2.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 94: Gradient norm: 12.9797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 24/24 [00:03<00:00,  6.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10\n",
            "Train Loss: 3.6484, Train Acc: 35.12%\n",
            "Val Loss: 4.8710, Val Acc: 14.08%\n",
            "Saved best model with Val Acc: 14.08%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/94 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
            "Batch 1: Outputs shape: torch.Size([32, 4902])\n",
            "Batch 1: Loss value: 3.1591529846191406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:42<00:00,  2.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 94: Gradient norm: 9.0046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 24/24 [00:03<00:00,  6.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10\n",
            "Train Loss: 2.7001, Train Acc: 54.39%\n",
            "Val Loss: 4.2958, Val Acc: 20.11%\n",
            "Saved best model with Val Acc: 20.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/94 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
            "Batch 1: Outputs shape: torch.Size([32, 4902])\n",
            "Batch 1: Loss value: 1.5195465087890625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:41<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 94: Gradient norm: 8.4424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 24/24 [00:03<00:00,  6.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10\n",
            "Train Loss: 2.0049, Train Acc: 68.24%\n",
            "Val Loss: 3.8728, Val Acc: 27.61%\n",
            "Saved best model with Val Acc: 27.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/94 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
            "Batch 1: Outputs shape: torch.Size([32, 4902])\n",
            "Batch 1: Loss value: 1.4219207763671875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:41<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 94: Gradient norm: 7.3370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 24/24 [00:03<00:00,  6.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10\n",
            "Train Loss: 1.5091, Train Acc: 78.30%\n",
            "Val Loss: 3.5670, Val Acc: 32.84%\n",
            "Saved best model with Val Acc: 32.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/94 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
            "Batch 1: Outputs shape: torch.Size([32, 4902])\n",
            "Batch 1: Loss value: 1.2771577835083008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:41<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 94: Gradient norm: 10.3836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 24/24 [00:03<00:00,  6.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10\n",
            "Train Loss: 1.1924, Train Acc: 84.91%\n",
            "Val Loss: 3.4411, Val Acc: 36.06%\n",
            "Saved best model with Val Acc: 36.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/94 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
            "Batch 1: Outputs shape: torch.Size([32, 4902])\n",
            "Batch 1: Loss value: 1.1783485412597656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:42<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 94: Gradient norm: 9.7003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 24/24 [00:03<00:00,  6.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10\n",
            "Train Loss: 1.0584, Train Acc: 87.96%\n",
            "Val Loss: 3.3672, Val Acc: 37.80%\n",
            "Saved best model with Val Acc: 37.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/94 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
            "Batch 1: Outputs shape: torch.Size([32, 4902])\n",
            "Batch 1: Loss value: 1.4635391235351562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 94/94 [00:41<00:00,  2.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 94: Gradient norm: 7.9621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 24/24 [00:03<00:00,  6.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10\n",
            "Train Loss: 0.9679, Train Acc: 89.70%\n",
            "Val Loss: 3.3498, Val Acc: 37.53%\n",
            "Training complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 14: THIS IS TO EXTRACT EMBEDDINGS TO COMPUTE SIMILARITY"
      ],
      "metadata": {
        "id": "f-VSuDjKBoNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_embeddings(model, loader, device):\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "    indices = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, lbls, idxs in tqdm(loader, desc=\"Extracting embeddings\"):\n",
        "            images = images.to(device)\n",
        "            # Get embeddings (remove the classifier head)\n",
        "            features = model.forward_features(images)  # For Swin Transformer\n",
        "\n",
        "            # The shape of features is [batch_size, 7, 7, 1024]\n",
        "            # We need to reshape it to [batch_size, 7*7*1024]\n",
        "            batch_size = features.size(0)\n",
        "            features = features.reshape(batch_size, -1)  # Flatten to [batch_size, 7*7*1024]\n",
        "\n",
        "            embeddings.append(features.cpu())\n",
        "            labels.append(lbls)\n",
        "            indices.append(idxs)\n",
        "\n",
        "    embeddings = torch.cat(embeddings, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    indices = torch.cat(indices, dim=0)\n",
        "    return embeddings, labels, indices\n",
        "\n",
        "# Extract embeddings for reference and validation sets\n",
        "reference_embeddings, reference_labels, reference_indices = extract_embeddings(model, reference_loader, device)\n",
        "val_embeddings, val_labels, val_indices = extract_embeddings(model, val_loader, device)\n",
        "\n",
        "print(f\"Reference embeddings shape: {reference_embeddings.shape}\")\n",
        "print(f\"Validation embeddings shape: {val_embeddings.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui9bRzbtCl23",
        "outputId": "e8069c31-ef6f-4412-fd0a-2302203abfd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting embeddings: 100%|██████████| 307/307 [02:01<00:00,  2.53it/s]\n",
            "Extracting embeddings: 100%|██████████| 24/24 [00:09<00:00,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference embeddings shape: torch.Size([9804, 7, 7, 1024])\n",
            "Validation embeddings shape: torch.Size([746, 7, 7, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 15: THIS IS TO COMPUTE METRICS"
      ],
      "metadata": {
        "id": "_vRtc1CYCt_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_cosine_similarity(query_emb, gallery_emb):\n",
        "    # Normalize embeddings for cosine similarity\n",
        "    query_emb = F.normalize(query_emb, p=2, dim=1)\n",
        "    gallery_emb = F.normalize(gallery_emb, p=2, dim=1)\n",
        "\n",
        "    # Compute dot product between normalized embeddings\n",
        "    similarity = torch.matmul(query_emb, gallery_emb.T)\n",
        "    return similarity\n",
        "\n",
        "def compute_retrieval_metrics(query_emb, query_labels, gallery_emb, gallery_labels, k_values=[2, 3, 4, 5]):\n",
        "    similarity = compute_cosine_similarity(query_emb, gallery_emb)\n",
        "    num_queries = query_emb.size(0)\n",
        "\n",
        "    top_1_correct = 0\n",
        "    top_k_correct = {k: 0 for k in k_values}\n",
        "    total_tp, total_fp, total_tn, total_fn = 0, 0, 0, 0\n",
        "\n",
        "    for i in tqdm(range(num_queries), desc=\"Computing metrics\"):\n",
        "        query_label = query_labels[i].item()\n",
        "        scores = similarity[i].cpu().numpy()\n",
        "        sorted_indices = np.argsort(scores)[::-1]  # Descending order\n",
        "\n",
        "        # Get labels of top-k retrieved images\n",
        "        retrieved_labels = gallery_labels.cpu().numpy()[sorted_indices]\n",
        "        true_label_mask = (retrieved_labels == query_label)\n",
        "\n",
        "        # Top-1 Accuracy\n",
        "        if retrieved_labels[0] == query_label:\n",
        "            top_1_correct += 1\n",
        "\n",
        "        # Top-k Accuracy\n",
        "        for k in k_values:\n",
        "            if np.any(true_label_mask[:k]):\n",
        "                top_k_correct[k] += 1\n",
        "\n",
        "        # TP, FP, TN, FN for k=5\n",
        "        k = 5\n",
        "        top_k_retrieved = true_label_mask[:k]\n",
        "        tp = np.sum(top_k_retrieved)  # Correctly retrieved\n",
        "        fp = k - tp  # Incorrectly retrieved\n",
        "\n",
        "        # Compute FN: Correct labels not in top-k\n",
        "        relevant_indices = np.where(gallery_labels.cpu().numpy() == query_label)[0]\n",
        "        fn = len(relevant_indices) - tp\n",
        "\n",
        "        # Compute TN: Non-relevant images not in top-k\n",
        "        # This is approximate to avoid computing all negatives\n",
        "        non_relevant_indices = np.where(gallery_labels.cpu().numpy() != query_label)[0]\n",
        "        retrieved_set = set(sorted_indices[:k])\n",
        "        tn = len([idx for idx in non_relevant_indices if idx not in retrieved_set])\n",
        "\n",
        "        total_tp += tp\n",
        "        total_fp += fp\n",
        "        total_tn += tn\n",
        "        total_fn += fn\n",
        "\n",
        "    # Compute accuracies\n",
        "    top_1_accuracy = top_1_correct / num_queries\n",
        "    top_k_accuracy = {k: correct / num_queries for k, correct in top_k_correct.items()}\n",
        "\n",
        "    # Aggregate confusion matrix metrics\n",
        "    confusion_metrics = {\n",
        "        'TP': total_tp,\n",
        "        'FP': total_fp,\n",
        "        'TN': total_tn,\n",
        "        'FN': total_fn\n",
        "    }\n",
        "\n",
        "    return top_1_accuracy, top_k_accuracy, confusion_metrics\n",
        "\n",
        "# Make sure embeddings are flattened properly\n",
        "print(f\"Checking reference embeddings shape: {reference_embeddings.shape}\")\n",
        "print(f\"Checking validation embeddings shape: {val_embeddings.shape}\")\n",
        "\n",
        "# Compute metrics\n",
        "top_1_accuracy, top_k_accuracy, confusion_metrics = compute_retrieval_metrics(\n",
        "    val_embeddings, val_labels,\n",
        "    reference_embeddings, reference_labels,\n",
        "    k_values=[2, 3, 4, 5]\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(f\"Top-1 Accuracy: {top_1_accuracy:.4f}\")\n",
        "for k, acc in top_k_accuracy.items():\n",
        "    print(f\"Top-{k} Accuracy: {acc:.4f}\")\n",
        "print(\"\\nConfusion Matrix Metrics (k=5):\")\n",
        "print(f\"True Positives: {confusion_metrics['TP']}\")\n",
        "print(f\"False Positives: {confusion_metrics['FP']}\")\n",
        "print(f\"True Negatives: {confusion_metrics['TN']}\")\n",
        "print(f\"False Negatives: {confusion_metrics['FN']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "7epiPMUaCw2W",
        "outputId": "2ab1476e-3c3e-40d4-b0a8-ad59ebd6ce82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-1307846920ce>:9: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3725.)\n",
            "  similarity = torch.matmul(query_emb, gallery_emb.T)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (746) must match the size of tensor b (1024) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1307846920ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Compute metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m top_1_accuracy, top_k_accuracy, confusion_metrics = compute_retrieval_metrics(\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mval_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mreference_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-1307846920ce>\u001b[0m in \u001b[0;36mcompute_retrieval_metrics\u001b[0;34m(query_emb, query_labels, gallery_emb, gallery_labels, k_values)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_retrieval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mnum_queries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-1307846920ce>\u001b[0m in \u001b[0;36mcompute_cosine_similarity\u001b[0;34m(query_emb, gallery_emb)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mquery_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgallery_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgallery_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgallery_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (746) must match the size of tensor b (1024) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 16: THIS IS SHAPE BASED PERFORMANCE ANALYSIS"
      ],
      "metadata": {
        "id": "_3XcxEKXDAQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Placeholder shape column (replace with actual shape data if available)\n",
        "shape_types = ['round', 'capsule', 'oval', 'square']  # Example shapes\n",
        "if 'shape' not in fixed_metadata.columns:\n",
        "    np.random.seed(42)\n",
        "    fixed_metadata['shape'] = np.random.choice(shape_types, size=len(fixed_metadata))\n",
        "\n",
        "# Filter validation metadata\n",
        "val_metadata = fixed_metadata[fixed_metadata.index.isin(val_data.index)]\n",
        "\n",
        "# Compute metrics by shape\n",
        "shape_results = {}\n",
        "for shape in shape_types:\n",
        "    # Filter validation embeddings and labels for this shape\n",
        "    shape_indices = val_metadata[val_metadata['shape'] == shape].index\n",
        "    valid_mask = np.isin(val_indices.cpu().numpy(), shape_indices)\n",
        "\n",
        "    if sum(valid_mask) == 0:\n",
        "        print(f\"No validation samples for shape: {shape}\")\n",
        "        continue\n",
        "\n",
        "    shape_val_emb = val_embeddings[valid_mask]\n",
        "    shape_val_labels = val_labels[valid_mask]\n",
        "\n",
        "    # Compute metrics using the function from Cell 15\n",
        "    top_1_acc, top_k_acc, conf_metrics = compute_retrieval_metrics(\n",
        "        shape_val_emb, shape_val_labels,\n",
        "        reference_embeddings, reference_labels,\n",
        "        k_values=[2, 3, 4, 5]\n",
        "    )\n",
        "\n",
        "    shape_results[shape] = {\n",
        "        'Top-1 Accuracy': top_1_acc,\n",
        "        'Top-2 Accuracy': top_k_acc[2],\n",
        "        'Top-3 Accuracy': top_k_acc[3],\n",
        "        'Top-4 Accuracy': top_k_acc[4],\n",
        "        'Top-5 Accuracy': top_k_acc[5],\n",
        "        'True Positives': conf_metrics['TP'],\n",
        "        'False Positives': conf_metrics['FP'],\n",
        "        'True Negatives': conf_metrics['TN'],\n",
        "        'False Negatives': conf_metrics['FN'],\n",
        "        'Num Samples': sum(valid_mask)\n",
        "    }\n",
        "\n",
        "    print(f\"Shape: {shape} ({sum(valid_mask)} samples)\")\n",
        "    print(f\"  Top-1 Accuracy: {top_1_acc:.4f}\")\n",
        "    for k in [2, 3, 4, 5]:\n",
        "        print(f\"  Top-{k} Accuracy: {top_k_acc[k]:.4f}\")\n",
        "    print(f\"  Confusion Matrix (k=5): TP={conf_metrics['TP']}, FP={conf_metrics['FP']}, TN={conf_metrics['TN']}, FN={conf_metrics['FN']}\")\n",
        "\n",
        "# Save results to CSV\n",
        "results_df = pd.DataFrame.from_dict(shape_results, orient='index')\n",
        "results_df.to_csv(\"shape_based_results.csv\")\n",
        "print(\"Shape-based results saved to shape_based_results.csv\")"
      ],
      "metadata": {
        "id": "9r-HOXmsDW__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 17: THIS IS TO COMPILE AND SAVE METRICS TO JSON FILE"
      ],
      "metadata": {
        "id": "TvrnVOIHDXtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "# Compile results\n",
        "results = {\n",
        "    'overall': {\n",
        "        'Top-1 Accuracy': float(top_1_accuracy),\n",
        "        'Top-k Accuracy': {str(k): float(acc) for k, acc in top_k_accuracy.items()},\n",
        "        'Confusion Metrics': {\n",
        "            'True Positives': int(confusion_metrics['TP']),\n",
        "            'False Positives': int(confusion_metrics['FP']),\n",
        "            'True Negatives': int(confusion_metrics['TN']),\n",
        "            'False Negatives': int(confusion_metrics['FN'])\n",
        "        },\n",
        "        'Num Validation Samples': len(val_dataset)\n",
        "    },\n",
        "    'by_shape': {\n",
        "        shape: {\n",
        "            'Top-1 Accuracy': float(metrics['Top-1 Accuracy']),\n",
        "            'Top-k Accuracy': {\n",
        "                '2': float(metrics['Top-2 Accuracy']),\n",
        "                '3': float(metrics['Top-3 Accuracy']),\n",
        "                '4': float(metrics['Top-4 Accuracy']),\n",
        "                '5': float(metrics['Top-5 Accuracy'])\n",
        "            },\n",
        "            'Confusion Metrics': {\n",
        "                'True Positives': int(metrics['True Positives']),\n",
        "                'False Positives': int(metrics['False Positives']),\n",
        "                'True Negatives': int(metrics['True Negatives']),\n",
        "                'False Negatives': int(metrics['False Negatives'])\n",
        "            },\n",
        "            'Num Samples': int(metrics['Num Samples'])\n",
        "        } for shape, metrics in shape_results.items()\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open('evaluation_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(\"Results saved to evaluation_results.json\")\n",
        "\n",
        "# Save the final model\n",
        "final_model_path = \"./final_swin_model.pth\"\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"Final model saved to {final_model_path}\")"
      ],
      "metadata": {
        "id": "DKqnqMMZDipI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 18: THIS IS TO VISUALIZE RESULTS"
      ],
      "metadata": {
        "id": "z28LyIKTDnNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def visualize_retrieval(query_idx, val_dataset, reference_dataset, similarity, top_k=5):\n",
        "    query_img, query_label, query_meta_idx = val_dataset[query_idx]\n",
        "    query_pill_id = fixed_metadata.iloc[query_meta_idx]['pilltype_id']\n",
        "\n",
        "    # Get top-k reference indices\n",
        "    scores = similarity[query_idx].cpu().numpy()\n",
        "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, top_k + 1, figsize=(15, 3))\n",
        "    axes[0].imshow(query_img.permute(1, 2, 0).numpy() * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
        "    axes[0].set_title(f\"Query: {query_pill_id}\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    for i, ref_idx in enumerate(top_indices):\n",
        "        ref_img, ref_label, ref_meta_idx = reference_dataset[ref_idx]\n",
        "        ref_pill_id = fixed_metadata.iloc[ref_meta_idx]['pilltype_id']\n",
        "        axes[i + 1].imshow(ref_img.permute(1, 2, 0).numpy() * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
        "        axes[i + 1].set_title(f\"Rank {i+1}: {ref_pill_id}\\nScore: {scores[ref_idx]:.4f}\")\n",
        "        axes[i + 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"retrieval_example_{query_idx}.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize a few examples\n",
        "similarity = compute_cosine_similarity(val_embeddings, reference_embeddings)\n",
        "for i in range(3):  # Show 3 examples\n",
        "    visualize_retrieval(i, val_dataset, reference_dataset, similarity)"
      ],
      "metadata": {
        "id": "G2ENHpR6Dspe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}